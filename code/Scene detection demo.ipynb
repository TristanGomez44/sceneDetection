{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scene detection demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import formatData\n",
    "import utils\n",
    "import subprocess\n",
    "import gdown\n",
    "import modelBuilder\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pims\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Getting key-frame indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "videoPath = \"../data/OVSD/Sintel.mp4\" # Put the path to your file here\n",
    "shotThres = 0.1                               # The shot detection threshold. Higher will produce less shot\n",
    "\n",
    "videoName = os.path.basename(os.path.splitext(videoPath)[0])\n",
    "\n",
    "if not os.path.exists(\"{}_shotBounds.csv\".format(videoName)):\n",
    "    nbFrames = formatData.getNbFrames(videoPath)\n",
    "    fps = utils.getVideoFPS(videoPath)\n",
    "\n",
    "    #Detecting shots boundaries. This can be a bit long\n",
    "    shotBounds = formatData.detect_format_shots(videoPath,shotThres,nbFrames,fps)\n",
    "       \n",
    "    #Saving shot boundaries\n",
    "    np.savetxt(\"{}_shotBounds.csv\".format(videoName),shotBounds)\n",
    "else:\n",
    "    shotBounds = np.genfromtxt(\"{}_shotBounds.csv\".format(videoName))\n",
    "    \n",
    "keyFrameInds = shotBounds.mean(axis=1).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "featModel.conv1.weight torch.Size([64, 3, 7, 7])\n",
      "featModel.bn1.weight torch.Size([64])\n",
      "featModel.bn1.bias torch.Size([64])\n",
      "featModel.bn1.running_mean torch.Size([64])\n",
      "featModel.bn1.running_var torch.Size([64])\n",
      "featModel.bn1.num_batches_tracked torch.Size([])\n",
      "featModel.layer1.0.conv1.weight torch.Size([64, 64, 1, 1])\n",
      "featModel.layer1.0.bn1.weight torch.Size([64])\n",
      "featModel.layer1.0.bn1.bias torch.Size([64])\n",
      "featModel.layer1.0.bn1.running_mean torch.Size([64])\n",
      "featModel.layer1.0.bn1.running_var torch.Size([64])\n",
      "featModel.layer1.0.bn1.num_batches_tracked torch.Size([])\n",
      "featModel.layer1.0.conv2.weight torch.Size([64, 64, 3, 3])\n",
      "featModel.layer1.0.bn2.weight torch.Size([64])\n",
      "featModel.layer1.0.bn2.bias torch.Size([64])\n",
      "featModel.layer1.0.bn2.running_mean torch.Size([64])\n",
      "featModel.layer1.0.bn2.running_var torch.Size([64])\n",
      "featModel.layer1.0.bn2.num_batches_tracked torch.Size([])\n",
      "featModel.layer1.0.conv3.weight torch.Size([256, 64, 1, 1])\n",
      "featModel.layer1.0.bn3.weight torch.Size([256])\n",
      "featModel.layer1.0.bn3.bias torch.Size([256])\n",
      "featModel.layer1.0.bn3.running_mean torch.Size([256])\n",
      "featModel.layer1.0.bn3.running_var torch.Size([256])\n",
      "featModel.layer1.0.bn3.num_batches_tracked torch.Size([])\n",
      "featModel.layer1.0.downsample.0.weight torch.Size([256, 64, 1, 1])\n",
      "featModel.layer1.0.downsample.1.weight torch.Size([256])\n",
      "featModel.layer1.0.downsample.1.bias torch.Size([256])\n",
      "featModel.layer1.0.downsample.1.running_mean torch.Size([256])\n",
      "featModel.layer1.0.downsample.1.running_var torch.Size([256])\n",
      "featModel.layer1.0.downsample.1.num_batches_tracked torch.Size([])\n",
      "featModel.layer1.1.conv1.weight torch.Size([64, 256, 1, 1])\n",
      "featModel.layer1.1.bn1.weight torch.Size([64])\n",
      "featModel.layer1.1.bn1.bias torch.Size([64])\n",
      "featModel.layer1.1.bn1.running_mean torch.Size([64])\n",
      "featModel.layer1.1.bn1.running_var torch.Size([64])\n",
      "featModel.layer1.1.bn1.num_batches_tracked torch.Size([])\n",
      "featModel.layer1.1.conv2.weight torch.Size([64, 64, 3, 3])\n",
      "featModel.layer1.1.bn2.weight torch.Size([64])\n",
      "featModel.layer1.1.bn2.bias torch.Size([64])\n",
      "featModel.layer1.1.bn2.running_mean torch.Size([64])\n",
      "featModel.layer1.1.bn2.running_var torch.Size([64])\n",
      "featModel.layer1.1.bn2.num_batches_tracked torch.Size([])\n",
      "featModel.layer1.1.conv3.weight torch.Size([256, 64, 1, 1])\n",
      "featModel.layer1.1.bn3.weight torch.Size([256])\n",
      "featModel.layer1.1.bn3.bias torch.Size([256])\n",
      "featModel.layer1.1.bn3.running_mean torch.Size([256])\n",
      "featModel.layer1.1.bn3.running_var torch.Size([256])\n",
      "featModel.layer1.1.bn3.num_batches_tracked torch.Size([])\n",
      "featModel.layer1.2.conv1.weight torch.Size([64, 256, 1, 1])\n",
      "featModel.layer1.2.bn1.weight torch.Size([64])\n",
      "featModel.layer1.2.bn1.bias torch.Size([64])\n",
      "featModel.layer1.2.bn1.running_mean torch.Size([64])\n",
      "featModel.layer1.2.bn1.running_var torch.Size([64])\n",
      "featModel.layer1.2.bn1.num_batches_tracked torch.Size([])\n",
      "featModel.layer1.2.conv2.weight torch.Size([64, 64, 3, 3])\n",
      "featModel.layer1.2.bn2.weight torch.Size([64])\n",
      "featModel.layer1.2.bn2.bias torch.Size([64])\n",
      "featModel.layer1.2.bn2.running_mean torch.Size([64])\n",
      "featModel.layer1.2.bn2.running_var torch.Size([64])\n",
      "featModel.layer1.2.bn2.num_batches_tracked torch.Size([])\n",
      "featModel.layer1.2.conv3.weight torch.Size([256, 64, 1, 1])\n",
      "featModel.layer1.2.bn3.weight torch.Size([256])\n",
      "featModel.layer1.2.bn3.bias torch.Size([256])\n",
      "featModel.layer1.2.bn3.running_mean torch.Size([256])\n",
      "featModel.layer1.2.bn3.running_var torch.Size([256])\n",
      "featModel.layer1.2.bn3.num_batches_tracked torch.Size([])\n",
      "featModel.layer2.0.conv1.weight torch.Size([128, 256, 1, 1])\n",
      "featModel.layer2.0.bn1.weight torch.Size([128])\n",
      "featModel.layer2.0.bn1.bias torch.Size([128])\n",
      "featModel.layer2.0.bn1.running_mean torch.Size([128])\n",
      "featModel.layer2.0.bn1.running_var torch.Size([128])\n",
      "featModel.layer2.0.bn1.num_batches_tracked torch.Size([])\n",
      "featModel.layer2.0.conv2.weight torch.Size([128, 128, 3, 3])\n",
      "featModel.layer2.0.bn2.weight torch.Size([128])\n",
      "featModel.layer2.0.bn2.bias torch.Size([128])\n",
      "featModel.layer2.0.bn2.running_mean torch.Size([128])\n",
      "featModel.layer2.0.bn2.running_var torch.Size([128])\n",
      "featModel.layer2.0.bn2.num_batches_tracked torch.Size([])\n",
      "featModel.layer2.0.conv3.weight torch.Size([512, 128, 1, 1])\n",
      "featModel.layer2.0.bn3.weight torch.Size([512])\n",
      "featModel.layer2.0.bn3.bias torch.Size([512])\n",
      "featModel.layer2.0.bn3.running_mean torch.Size([512])\n",
      "featModel.layer2.0.bn3.running_var torch.Size([512])\n",
      "featModel.layer2.0.bn3.num_batches_tracked torch.Size([])\n",
      "featModel.layer2.0.downsample.0.weight torch.Size([512, 256, 1, 1])\n",
      "featModel.layer2.0.downsample.1.weight torch.Size([512])\n",
      "featModel.layer2.0.downsample.1.bias torch.Size([512])\n",
      "featModel.layer2.0.downsample.1.running_mean torch.Size([512])\n",
      "featModel.layer2.0.downsample.1.running_var torch.Size([512])\n",
      "featModel.layer2.0.downsample.1.num_batches_tracked torch.Size([])\n",
      "featModel.layer2.1.conv1.weight torch.Size([128, 512, 1, 1])\n",
      "featModel.layer2.1.bn1.weight torch.Size([128])\n",
      "featModel.layer2.1.bn1.bias torch.Size([128])\n",
      "featModel.layer2.1.bn1.running_mean torch.Size([128])\n",
      "featModel.layer2.1.bn1.running_var torch.Size([128])\n",
      "featModel.layer2.1.bn1.num_batches_tracked torch.Size([])\n",
      "featModel.layer2.1.conv2.weight torch.Size([128, 128, 3, 3])\n",
      "featModel.layer2.1.bn2.weight torch.Size([128])\n",
      "featModel.layer2.1.bn2.bias torch.Size([128])\n",
      "featModel.layer2.1.bn2.running_mean torch.Size([128])\n",
      "featModel.layer2.1.bn2.running_var torch.Size([128])\n",
      "featModel.layer2.1.bn2.num_batches_tracked torch.Size([])\n",
      "featModel.layer2.1.conv3.weight torch.Size([512, 128, 1, 1])\n",
      "featModel.layer2.1.bn3.weight torch.Size([512])\n",
      "featModel.layer2.1.bn3.bias torch.Size([512])\n",
      "featModel.layer2.1.bn3.running_mean torch.Size([512])\n",
      "featModel.layer2.1.bn3.running_var torch.Size([512])\n",
      "featModel.layer2.1.bn3.num_batches_tracked torch.Size([])\n",
      "featModel.layer2.2.conv1.weight torch.Size([128, 512, 1, 1])\n",
      "featModel.layer2.2.bn1.weight torch.Size([128])\n",
      "featModel.layer2.2.bn1.bias torch.Size([128])\n",
      "featModel.layer2.2.bn1.running_mean torch.Size([128])\n",
      "featModel.layer2.2.bn1.running_var torch.Size([128])\n",
      "featModel.layer2.2.bn1.num_batches_tracked torch.Size([])\n",
      "featModel.layer2.2.conv2.weight torch.Size([128, 128, 3, 3])\n",
      "featModel.layer2.2.bn2.weight torch.Size([128])\n",
      "featModel.layer2.2.bn2.bias torch.Size([128])\n",
      "featModel.layer2.2.bn2.running_mean torch.Size([128])\n",
      "featModel.layer2.2.bn2.running_var torch.Size([128])\n",
      "featModel.layer2.2.bn2.num_batches_tracked torch.Size([])\n",
      "featModel.layer2.2.conv3.weight torch.Size([512, 128, 1, 1])\n",
      "featModel.layer2.2.bn3.weight torch.Size([512])\n",
      "featModel.layer2.2.bn3.bias torch.Size([512])\n",
      "featModel.layer2.2.bn3.running_mean torch.Size([512])\n",
      "featModel.layer2.2.bn3.running_var torch.Size([512])\n",
      "featModel.layer2.2.bn3.num_batches_tracked torch.Size([])\n",
      "featModel.layer2.3.conv1.weight torch.Size([128, 512, 1, 1])\n",
      "featModel.layer2.3.bn1.weight torch.Size([128])\n",
      "featModel.layer2.3.bn1.bias torch.Size([128])\n",
      "featModel.layer2.3.bn1.running_mean torch.Size([128])\n",
      "featModel.layer2.3.bn1.running_var torch.Size([128])\n",
      "featModel.layer2.3.bn1.num_batches_tracked torch.Size([])\n",
      "featModel.layer2.3.conv2.weight torch.Size([128, 128, 3, 3])\n",
      "featModel.layer2.3.bn2.weight torch.Size([128])\n",
      "featModel.layer2.3.bn2.bias torch.Size([128])\n",
      "featModel.layer2.3.bn2.running_mean torch.Size([128])\n",
      "featModel.layer2.3.bn2.running_var torch.Size([128])\n",
      "featModel.layer2.3.bn2.num_batches_tracked torch.Size([])\n",
      "featModel.layer2.3.conv3.weight torch.Size([512, 128, 1, 1])\n",
      "featModel.layer2.3.bn3.weight torch.Size([512])\n",
      "featModel.layer2.3.bn3.bias torch.Size([512])\n",
      "featModel.layer2.3.bn3.running_mean torch.Size([512])\n",
      "featModel.layer2.3.bn3.running_var torch.Size([512])\n",
      "featModel.layer2.3.bn3.num_batches_tracked torch.Size([])\n",
      "featModel.layer3.0.conv1.weight torch.Size([256, 512, 1, 1])\n",
      "featModel.layer3.0.bn1.weight torch.Size([256])\n",
      "featModel.layer3.0.bn1.bias torch.Size([256])\n",
      "featModel.layer3.0.bn1.running_mean torch.Size([256])\n",
      "featModel.layer3.0.bn1.running_var torch.Size([256])\n",
      "featModel.layer3.0.bn1.num_batches_tracked torch.Size([])\n",
      "featModel.layer3.0.conv2.weight torch.Size([256, 256, 3, 3])\n",
      "featModel.layer3.0.bn2.weight torch.Size([256])\n",
      "featModel.layer3.0.bn2.bias torch.Size([256])\n",
      "featModel.layer3.0.bn2.running_mean torch.Size([256])\n",
      "featModel.layer3.0.bn2.running_var torch.Size([256])\n",
      "featModel.layer3.0.bn2.num_batches_tracked torch.Size([])\n",
      "featModel.layer3.0.conv3.weight torch.Size([1024, 256, 1, 1])\n",
      "featModel.layer3.0.bn3.weight torch.Size([1024])\n",
      "featModel.layer3.0.bn3.bias torch.Size([1024])\n",
      "featModel.layer3.0.bn3.running_mean torch.Size([1024])\n",
      "featModel.layer3.0.bn3.running_var torch.Size([1024])\n",
      "featModel.layer3.0.bn3.num_batches_tracked torch.Size([])\n",
      "featModel.layer3.0.downsample.0.weight torch.Size([1024, 512, 1, 1])\n",
      "featModel.layer3.0.downsample.1.weight torch.Size([1024])\n",
      "featModel.layer3.0.downsample.1.bias torch.Size([1024])\n",
      "featModel.layer3.0.downsample.1.running_mean torch.Size([1024])\n",
      "featModel.layer3.0.downsample.1.running_var torch.Size([1024])\n",
      "featModel.layer3.0.downsample.1.num_batches_tracked torch.Size([])\n",
      "featModel.layer3.1.conv1.weight torch.Size([256, 1024, 1, 1])\n",
      "featModel.layer3.1.bn1.weight torch.Size([256])\n",
      "featModel.layer3.1.bn1.bias torch.Size([256])\n",
      "featModel.layer3.1.bn1.running_mean torch.Size([256])\n",
      "featModel.layer3.1.bn1.running_var torch.Size([256])\n",
      "featModel.layer3.1.bn1.num_batches_tracked torch.Size([])\n",
      "featModel.layer3.1.conv2.weight torch.Size([256, 256, 3, 3])\n",
      "featModel.layer3.1.bn2.weight torch.Size([256])\n",
      "featModel.layer3.1.bn2.bias torch.Size([256])\n",
      "featModel.layer3.1.bn2.running_mean torch.Size([256])\n",
      "featModel.layer3.1.bn2.running_var torch.Size([256])\n",
      "featModel.layer3.1.bn2.num_batches_tracked torch.Size([])\n",
      "featModel.layer3.1.conv3.weight torch.Size([1024, 256, 1, 1])\n",
      "featModel.layer3.1.bn3.weight torch.Size([1024])\n",
      "featModel.layer3.1.bn3.bias torch.Size([1024])\n",
      "featModel.layer3.1.bn3.running_mean torch.Size([1024])\n",
      "featModel.layer3.1.bn3.running_var torch.Size([1024])\n",
      "featModel.layer3.1.bn3.num_batches_tracked torch.Size([])\n",
      "featModel.layer3.2.conv1.weight torch.Size([256, 1024, 1, 1])\n",
      "featModel.layer3.2.bn1.weight torch.Size([256])\n",
      "featModel.layer3.2.bn1.bias torch.Size([256])\n",
      "featModel.layer3.2.bn1.running_mean torch.Size([256])\n",
      "featModel.layer3.2.bn1.running_var torch.Size([256])\n",
      "featModel.layer3.2.bn1.num_batches_tracked torch.Size([])\n",
      "featModel.layer3.2.conv2.weight torch.Size([256, 256, 3, 3])\n",
      "featModel.layer3.2.bn2.weight torch.Size([256])\n",
      "featModel.layer3.2.bn2.bias torch.Size([256])\n",
      "featModel.layer3.2.bn2.running_mean torch.Size([256])\n",
      "featModel.layer3.2.bn2.running_var torch.Size([256])\n",
      "featModel.layer3.2.bn2.num_batches_tracked torch.Size([])\n",
      "featModel.layer3.2.conv3.weight torch.Size([1024, 256, 1, 1])\n",
      "featModel.layer3.2.bn3.weight torch.Size([1024])\n",
      "featModel.layer3.2.bn3.bias torch.Size([1024])\n",
      "featModel.layer3.2.bn3.running_mean torch.Size([1024])\n",
      "featModel.layer3.2.bn3.running_var torch.Size([1024])\n",
      "featModel.layer3.2.bn3.num_batches_tracked torch.Size([])\n",
      "featModel.layer3.3.conv1.weight torch.Size([256, 1024, 1, 1])\n",
      "featModel.layer3.3.bn1.weight torch.Size([256])\n",
      "featModel.layer3.3.bn1.bias torch.Size([256])\n",
      "featModel.layer3.3.bn1.running_mean torch.Size([256])\n",
      "featModel.layer3.3.bn1.running_var torch.Size([256])\n",
      "featModel.layer3.3.bn1.num_batches_tracked torch.Size([])\n",
      "featModel.layer3.3.conv2.weight torch.Size([256, 256, 3, 3])\n",
      "featModel.layer3.3.bn2.weight torch.Size([256])\n",
      "featModel.layer3.3.bn2.bias torch.Size([256])\n",
      "featModel.layer3.3.bn2.running_mean torch.Size([256])\n",
      "featModel.layer3.3.bn2.running_var torch.Size([256])\n",
      "featModel.layer3.3.bn2.num_batches_tracked torch.Size([])\n",
      "featModel.layer3.3.conv3.weight torch.Size([1024, 256, 1, 1])\n",
      "featModel.layer3.3.bn3.weight torch.Size([1024])\n",
      "featModel.layer3.3.bn3.bias torch.Size([1024])\n",
      "featModel.layer3.3.bn3.running_mean torch.Size([1024])\n",
      "featModel.layer3.3.bn3.running_var torch.Size([1024])\n",
      "featModel.layer3.3.bn3.num_batches_tracked torch.Size([])\n",
      "featModel.layer3.4.conv1.weight torch.Size([256, 1024, 1, 1])\n",
      "featModel.layer3.4.bn1.weight torch.Size([256])\n",
      "featModel.layer3.4.bn1.bias torch.Size([256])\n",
      "featModel.layer3.4.bn1.running_mean torch.Size([256])\n",
      "featModel.layer3.4.bn1.running_var torch.Size([256])\n",
      "featModel.layer3.4.bn1.num_batches_tracked torch.Size([])\n",
      "featModel.layer3.4.conv2.weight torch.Size([256, 256, 3, 3])\n",
      "featModel.layer3.4.bn2.weight torch.Size([256])\n",
      "featModel.layer3.4.bn2.bias torch.Size([256])\n",
      "featModel.layer3.4.bn2.running_mean torch.Size([256])\n",
      "featModel.layer3.4.bn2.running_var torch.Size([256])\n",
      "featModel.layer3.4.bn2.num_batches_tracked torch.Size([])\n",
      "featModel.layer3.4.conv3.weight torch.Size([1024, 256, 1, 1])\n",
      "featModel.layer3.4.bn3.weight torch.Size([1024])\n",
      "featModel.layer3.4.bn3.bias torch.Size([1024])\n",
      "featModel.layer3.4.bn3.running_mean torch.Size([1024])\n",
      "featModel.layer3.4.bn3.running_var torch.Size([1024])\n",
      "featModel.layer3.4.bn3.num_batches_tracked torch.Size([])\n",
      "featModel.layer3.5.conv1.weight torch.Size([256, 1024, 1, 1])\n",
      "featModel.layer3.5.bn1.weight torch.Size([256])\n",
      "featModel.layer3.5.bn1.bias torch.Size([256])\n",
      "featModel.layer3.5.bn1.running_mean torch.Size([256])\n",
      "featModel.layer3.5.bn1.running_var torch.Size([256])\n",
      "featModel.layer3.5.bn1.num_batches_tracked torch.Size([])\n",
      "featModel.layer3.5.conv2.weight torch.Size([256, 256, 3, 3])\n",
      "featModel.layer3.5.bn2.weight torch.Size([256])\n",
      "featModel.layer3.5.bn2.bias torch.Size([256])\n",
      "featModel.layer3.5.bn2.running_mean torch.Size([256])\n",
      "featModel.layer3.5.bn2.running_var torch.Size([256])\n",
      "featModel.layer3.5.bn2.num_batches_tracked torch.Size([])\n",
      "featModel.layer3.5.conv3.weight torch.Size([1024, 256, 1, 1])\n",
      "featModel.layer3.5.bn3.weight torch.Size([1024])\n",
      "featModel.layer3.5.bn3.bias torch.Size([1024])\n",
      "featModel.layer3.5.bn3.running_mean torch.Size([1024])\n",
      "featModel.layer3.5.bn3.running_var torch.Size([1024])\n",
      "featModel.layer3.5.bn3.num_batches_tracked torch.Size([])\n",
      "featModel.layer4.0.conv1.weight torch.Size([512, 1024, 1, 1])\n",
      "featModel.layer4.0.bn1.weight torch.Size([512])\n",
      "featModel.layer4.0.bn1.bias torch.Size([512])\n",
      "featModel.layer4.0.bn1.running_mean torch.Size([512])\n",
      "featModel.layer4.0.bn1.running_var torch.Size([512])\n",
      "featModel.layer4.0.bn1.num_batches_tracked torch.Size([])\n",
      "featModel.layer4.0.conv2.weight torch.Size([512, 512, 3, 3])\n",
      "featModel.layer4.0.bn2.weight torch.Size([512])\n",
      "featModel.layer4.0.bn2.bias torch.Size([512])\n",
      "featModel.layer4.0.bn2.running_mean torch.Size([512])\n",
      "featModel.layer4.0.bn2.running_var torch.Size([512])\n",
      "featModel.layer4.0.bn2.num_batches_tracked torch.Size([])\n",
      "featModel.layer4.0.conv3.weight torch.Size([2048, 512, 1, 1])\n",
      "featModel.layer4.0.bn3.weight torch.Size([2048])\n",
      "featModel.layer4.0.bn3.bias torch.Size([2048])\n",
      "featModel.layer4.0.bn3.running_mean torch.Size([2048])\n",
      "featModel.layer4.0.bn3.running_var torch.Size([2048])\n",
      "featModel.layer4.0.bn3.num_batches_tracked torch.Size([])\n",
      "featModel.layer4.0.downsample.0.weight torch.Size([2048, 1024, 1, 1])\n",
      "featModel.layer4.0.downsample.1.weight torch.Size([2048])\n",
      "featModel.layer4.0.downsample.1.bias torch.Size([2048])\n",
      "featModel.layer4.0.downsample.1.running_mean torch.Size([2048])\n",
      "featModel.layer4.0.downsample.1.running_var torch.Size([2048])\n",
      "featModel.layer4.0.downsample.1.num_batches_tracked torch.Size([])\n",
      "featModel.layer4.1.conv1.weight torch.Size([512, 2048, 1, 1])\n",
      "featModel.layer4.1.bn1.weight torch.Size([512])\n",
      "featModel.layer4.1.bn1.bias torch.Size([512])\n",
      "featModel.layer4.1.bn1.running_mean torch.Size([512])\n",
      "featModel.layer4.1.bn1.running_var torch.Size([512])\n",
      "featModel.layer4.1.bn1.num_batches_tracked torch.Size([])\n",
      "featModel.layer4.1.conv2.weight torch.Size([512, 512, 3, 3])\n",
      "featModel.layer4.1.bn2.weight torch.Size([512])\n",
      "featModel.layer4.1.bn2.bias torch.Size([512])\n",
      "featModel.layer4.1.bn2.running_mean torch.Size([512])\n",
      "featModel.layer4.1.bn2.running_var torch.Size([512])\n",
      "featModel.layer4.1.bn2.num_batches_tracked torch.Size([])\n",
      "featModel.layer4.1.conv3.weight torch.Size([2048, 512, 1, 1])\n",
      "featModel.layer4.1.bn3.weight torch.Size([2048])\n",
      "featModel.layer4.1.bn3.bias torch.Size([2048])\n",
      "featModel.layer4.1.bn3.running_mean torch.Size([2048])\n",
      "featModel.layer4.1.bn3.running_var torch.Size([2048])\n",
      "featModel.layer4.1.bn3.num_batches_tracked torch.Size([])\n",
      "featModel.layer4.2.conv1.weight torch.Size([512, 2048, 1, 1])\n",
      "featModel.layer4.2.bn1.weight torch.Size([512])\n",
      "featModel.layer4.2.bn1.bias torch.Size([512])\n",
      "featModel.layer4.2.bn1.running_mean torch.Size([512])\n",
      "featModel.layer4.2.bn1.running_var torch.Size([512])\n",
      "featModel.layer4.2.bn1.num_batches_tracked torch.Size([])\n",
      "featModel.layer4.2.conv2.weight torch.Size([512, 512, 3, 3])\n",
      "featModel.layer4.2.bn2.weight torch.Size([512])\n",
      "featModel.layer4.2.bn2.bias torch.Size([512])\n",
      "featModel.layer4.2.bn2.running_mean torch.Size([512])\n",
      "featModel.layer4.2.bn2.running_var torch.Size([512])\n",
      "featModel.layer4.2.bn2.num_batches_tracked torch.Size([])\n",
      "featModel.layer4.2.conv3.weight torch.Size([2048, 512, 1, 1])\n",
      "featModel.layer4.2.bn3.weight torch.Size([2048])\n",
      "featModel.layer4.2.bn3.bias torch.Size([2048])\n",
      "featModel.layer4.2.bn3.running_mean torch.Size([2048])\n",
      "featModel.layer4.2.bn3.running_var torch.Size([2048])\n",
      "featModel.layer4.2.bn3.num_batches_tracked torch.Size([])\n",
      "featModel.fc.weight torch.Size([1000, 2048])\n",
      "featModel.fc.bias torch.Size([1000])\n",
      "tempModel.cnn.conv1.weight torch.Size([64, 3, 7, 7])\n",
      "tempModel.cnn.bn1.weight torch.Size([64])\n",
      "tempModel.cnn.bn1.bias torch.Size([64])\n",
      "tempModel.cnn.bn1.running_mean torch.Size([64])\n",
      "tempModel.cnn.bn1.running_var torch.Size([64])\n",
      "tempModel.cnn.bn1.num_batches_tracked torch.Size([])\n",
      "tempModel.cnn.layer1.0.conv1.weight torch.Size([64, 64, 1, 1])\n",
      "tempModel.cnn.layer1.0.bn1.weight torch.Size([64])\n",
      "tempModel.cnn.layer1.0.bn1.bias torch.Size([64])\n",
      "tempModel.cnn.layer1.0.bn1.running_mean torch.Size([64])\n",
      "tempModel.cnn.layer1.0.bn1.running_var torch.Size([64])\n",
      "tempModel.cnn.layer1.0.bn1.num_batches_tracked torch.Size([])\n",
      "tempModel.cnn.layer1.0.conv2.weight torch.Size([64, 64, 3, 3])\n",
      "tempModel.cnn.layer1.0.bn2.weight torch.Size([64])\n",
      "tempModel.cnn.layer1.0.bn2.bias torch.Size([64])\n",
      "tempModel.cnn.layer1.0.bn2.running_mean torch.Size([64])\n",
      "tempModel.cnn.layer1.0.bn2.running_var torch.Size([64])\n",
      "tempModel.cnn.layer1.0.bn2.num_batches_tracked torch.Size([])\n",
      "tempModel.cnn.layer1.0.conv3.weight torch.Size([256, 64, 1, 1])\n",
      "tempModel.cnn.layer1.0.bn3.weight torch.Size([256])\n",
      "tempModel.cnn.layer1.0.bn3.bias torch.Size([256])\n",
      "tempModel.cnn.layer1.0.bn3.running_mean torch.Size([256])\n",
      "tempModel.cnn.layer1.0.bn3.running_var torch.Size([256])\n",
      "tempModel.cnn.layer1.0.bn3.num_batches_tracked torch.Size([])\n",
      "tempModel.cnn.layer1.0.downsample.0.weight torch.Size([256, 64, 1, 1])\n",
      "tempModel.cnn.layer1.0.downsample.1.weight torch.Size([256])\n",
      "tempModel.cnn.layer1.0.downsample.1.bias torch.Size([256])\n",
      "tempModel.cnn.layer1.0.downsample.1.running_mean torch.Size([256])\n",
      "tempModel.cnn.layer1.0.downsample.1.running_var torch.Size([256])\n",
      "tempModel.cnn.layer1.0.downsample.1.num_batches_tracked torch.Size([])\n",
      "tempModel.cnn.layer1.1.conv1.weight torch.Size([64, 256, 1, 1])\n",
      "tempModel.cnn.layer1.1.bn1.weight torch.Size([64])\n",
      "tempModel.cnn.layer1.1.bn1.bias torch.Size([64])\n",
      "tempModel.cnn.layer1.1.bn1.running_mean torch.Size([64])\n",
      "tempModel.cnn.layer1.1.bn1.running_var torch.Size([64])\n",
      "tempModel.cnn.layer1.1.bn1.num_batches_tracked torch.Size([])\n",
      "tempModel.cnn.layer1.1.conv2.weight torch.Size([64, 64, 3, 3])\n",
      "tempModel.cnn.layer1.1.bn2.weight torch.Size([64])\n",
      "tempModel.cnn.layer1.1.bn2.bias torch.Size([64])\n",
      "tempModel.cnn.layer1.1.bn2.running_mean torch.Size([64])\n",
      "tempModel.cnn.layer1.1.bn2.running_var torch.Size([64])\n",
      "tempModel.cnn.layer1.1.bn2.num_batches_tracked torch.Size([])\n",
      "tempModel.cnn.layer1.1.conv3.weight torch.Size([256, 64, 1, 1])\n",
      "tempModel.cnn.layer1.1.bn3.weight torch.Size([256])\n",
      "tempModel.cnn.layer1.1.bn3.bias torch.Size([256])\n",
      "tempModel.cnn.layer1.1.bn3.running_mean torch.Size([256])\n",
      "tempModel.cnn.layer1.1.bn3.running_var torch.Size([256])\n",
      "tempModel.cnn.layer1.1.bn3.num_batches_tracked torch.Size([])\n",
      "tempModel.cnn.layer1.2.conv1.weight torch.Size([64, 256, 1, 1])\n",
      "tempModel.cnn.layer1.2.bn1.weight torch.Size([64])\n",
      "tempModel.cnn.layer1.2.bn1.bias torch.Size([64])\n",
      "tempModel.cnn.layer1.2.bn1.running_mean torch.Size([64])\n",
      "tempModel.cnn.layer1.2.bn1.running_var torch.Size([64])\n",
      "tempModel.cnn.layer1.2.bn1.num_batches_tracked torch.Size([])\n",
      "tempModel.cnn.layer1.2.conv2.weight torch.Size([64, 64, 3, 3])\n",
      "tempModel.cnn.layer1.2.bn2.weight torch.Size([64])\n",
      "tempModel.cnn.layer1.2.bn2.bias torch.Size([64])\n",
      "tempModel.cnn.layer1.2.bn2.running_mean torch.Size([64])\n",
      "tempModel.cnn.layer1.2.bn2.running_var torch.Size([64])\n",
      "tempModel.cnn.layer1.2.bn2.num_batches_tracked torch.Size([])\n",
      "tempModel.cnn.layer1.2.conv3.weight torch.Size([256, 64, 1, 1])\n",
      "tempModel.cnn.layer1.2.bn3.weight torch.Size([256])\n",
      "tempModel.cnn.layer1.2.bn3.bias torch.Size([256])\n",
      "tempModel.cnn.layer1.2.bn3.running_mean torch.Size([256])\n",
      "tempModel.cnn.layer1.2.bn3.running_var torch.Size([256])\n",
      "tempModel.cnn.layer1.2.bn3.num_batches_tracked torch.Size([])\n",
      "tempModel.cnn.layer2.0.conv1.weight torch.Size([128, 256, 1, 1])\n",
      "tempModel.cnn.layer2.0.bn1.weight torch.Size([128])\n",
      "tempModel.cnn.layer2.0.bn1.bias torch.Size([128])\n",
      "tempModel.cnn.layer2.0.bn1.running_mean torch.Size([128])\n",
      "tempModel.cnn.layer2.0.bn1.running_var torch.Size([128])\n",
      "tempModel.cnn.layer2.0.bn1.num_batches_tracked torch.Size([])\n",
      "tempModel.cnn.layer2.0.conv2.weight torch.Size([128, 128, 3, 3])\n",
      "tempModel.cnn.layer2.0.bn2.weight torch.Size([128])\n",
      "tempModel.cnn.layer2.0.bn2.bias torch.Size([128])\n",
      "tempModel.cnn.layer2.0.bn2.running_mean torch.Size([128])\n",
      "tempModel.cnn.layer2.0.bn2.running_var torch.Size([128])\n",
      "tempModel.cnn.layer2.0.bn2.num_batches_tracked torch.Size([])\n",
      "tempModel.cnn.layer2.0.conv3.weight torch.Size([512, 128, 1, 1])\n",
      "tempModel.cnn.layer2.0.bn3.weight torch.Size([512])\n",
      "tempModel.cnn.layer2.0.bn3.bias torch.Size([512])\n",
      "tempModel.cnn.layer2.0.bn3.running_mean torch.Size([512])\n",
      "tempModel.cnn.layer2.0.bn3.running_var torch.Size([512])\n",
      "tempModel.cnn.layer2.0.bn3.num_batches_tracked torch.Size([])\n",
      "tempModel.cnn.layer2.0.downsample.0.weight torch.Size([512, 256, 1, 1])\n",
      "tempModel.cnn.layer2.0.downsample.1.weight torch.Size([512])\n",
      "tempModel.cnn.layer2.0.downsample.1.bias torch.Size([512])\n",
      "tempModel.cnn.layer2.0.downsample.1.running_mean torch.Size([512])\n",
      "tempModel.cnn.layer2.0.downsample.1.running_var torch.Size([512])\n",
      "tempModel.cnn.layer2.0.downsample.1.num_batches_tracked torch.Size([])\n",
      "tempModel.cnn.layer2.1.conv1.weight torch.Size([128, 512, 1, 1])\n",
      "tempModel.cnn.layer2.1.bn1.weight torch.Size([128])\n",
      "tempModel.cnn.layer2.1.bn1.bias torch.Size([128])\n",
      "tempModel.cnn.layer2.1.bn1.running_mean torch.Size([128])\n",
      "tempModel.cnn.layer2.1.bn1.running_var torch.Size([128])\n",
      "tempModel.cnn.layer2.1.bn1.num_batches_tracked torch.Size([])\n",
      "tempModel.cnn.layer2.1.conv2.weight torch.Size([128, 128, 3, 3])\n",
      "tempModel.cnn.layer2.1.bn2.weight torch.Size([128])\n",
      "tempModel.cnn.layer2.1.bn2.bias torch.Size([128])\n",
      "tempModel.cnn.layer2.1.bn2.running_mean torch.Size([128])\n",
      "tempModel.cnn.layer2.1.bn2.running_var torch.Size([128])\n",
      "tempModel.cnn.layer2.1.bn2.num_batches_tracked torch.Size([])\n",
      "tempModel.cnn.layer2.1.conv3.weight torch.Size([512, 128, 1, 1])\n",
      "tempModel.cnn.layer2.1.bn3.weight torch.Size([512])\n",
      "tempModel.cnn.layer2.1.bn3.bias torch.Size([512])\n",
      "tempModel.cnn.layer2.1.bn3.running_mean torch.Size([512])\n",
      "tempModel.cnn.layer2.1.bn3.running_var torch.Size([512])\n",
      "tempModel.cnn.layer2.1.bn3.num_batches_tracked torch.Size([])\n",
      "tempModel.cnn.layer2.2.conv1.weight torch.Size([128, 512, 1, 1])\n",
      "tempModel.cnn.layer2.2.bn1.weight torch.Size([128])\n",
      "tempModel.cnn.layer2.2.bn1.bias torch.Size([128])\n",
      "tempModel.cnn.layer2.2.bn1.running_mean torch.Size([128])\n",
      "tempModel.cnn.layer2.2.bn1.running_var torch.Size([128])\n",
      "tempModel.cnn.layer2.2.bn1.num_batches_tracked torch.Size([])\n",
      "tempModel.cnn.layer2.2.conv2.weight torch.Size([128, 128, 3, 3])\n",
      "tempModel.cnn.layer2.2.bn2.weight torch.Size([128])\n",
      "tempModel.cnn.layer2.2.bn2.bias torch.Size([128])\n",
      "tempModel.cnn.layer2.2.bn2.running_mean torch.Size([128])\n",
      "tempModel.cnn.layer2.2.bn2.running_var torch.Size([128])\n",
      "tempModel.cnn.layer2.2.bn2.num_batches_tracked torch.Size([])\n",
      "tempModel.cnn.layer2.2.conv3.weight torch.Size([512, 128, 1, 1])\n",
      "tempModel.cnn.layer2.2.bn3.weight torch.Size([512])\n",
      "tempModel.cnn.layer2.2.bn3.bias torch.Size([512])\n",
      "tempModel.cnn.layer2.2.bn3.running_mean torch.Size([512])\n",
      "tempModel.cnn.layer2.2.bn3.running_var torch.Size([512])\n",
      "tempModel.cnn.layer2.2.bn3.num_batches_tracked torch.Size([])\n",
      "tempModel.cnn.layer2.3.conv1.weight torch.Size([128, 512, 1, 1])\n",
      "tempModel.cnn.layer2.3.bn1.weight torch.Size([128])\n",
      "tempModel.cnn.layer2.3.bn1.bias torch.Size([128])\n",
      "tempModel.cnn.layer2.3.bn1.running_mean torch.Size([128])\n",
      "tempModel.cnn.layer2.3.bn1.running_var torch.Size([128])\n",
      "tempModel.cnn.layer2.3.bn1.num_batches_tracked torch.Size([])\n",
      "tempModel.cnn.layer2.3.conv2.weight torch.Size([128, 128, 3, 3])\n",
      "tempModel.cnn.layer2.3.bn2.weight torch.Size([128])\n",
      "tempModel.cnn.layer2.3.bn2.bias torch.Size([128])\n",
      "tempModel.cnn.layer2.3.bn2.running_mean torch.Size([128])\n",
      "tempModel.cnn.layer2.3.bn2.running_var torch.Size([128])\n",
      "tempModel.cnn.layer2.3.bn2.num_batches_tracked torch.Size([])\n",
      "tempModel.cnn.layer2.3.conv3.weight torch.Size([512, 128, 1, 1])\n",
      "tempModel.cnn.layer2.3.bn3.weight torch.Size([512])\n",
      "tempModel.cnn.layer2.3.bn3.bias torch.Size([512])\n",
      "tempModel.cnn.layer2.3.bn3.running_mean torch.Size([512])\n",
      "tempModel.cnn.layer2.3.bn3.running_var torch.Size([512])\n",
      "tempModel.cnn.layer2.3.bn3.num_batches_tracked torch.Size([])\n",
      "tempModel.cnn.layer3.0.conv1.weight torch.Size([256, 512, 1, 1])\n",
      "tempModel.cnn.layer3.0.bn1.weight torch.Size([256])\n",
      "tempModel.cnn.layer3.0.bn1.bias torch.Size([256])\n",
      "tempModel.cnn.layer3.0.bn1.running_mean torch.Size([256])\n",
      "tempModel.cnn.layer3.0.bn1.running_var torch.Size([256])\n",
      "tempModel.cnn.layer3.0.bn1.num_batches_tracked torch.Size([])\n",
      "tempModel.cnn.layer3.0.conv2.weight torch.Size([256, 256, 3, 3])\n",
      "tempModel.cnn.layer3.0.bn2.weight torch.Size([256])\n",
      "tempModel.cnn.layer3.0.bn2.bias torch.Size([256])\n",
      "tempModel.cnn.layer3.0.bn2.running_mean torch.Size([256])\n",
      "tempModel.cnn.layer3.0.bn2.running_var torch.Size([256])\n",
      "tempModel.cnn.layer3.0.bn2.num_batches_tracked torch.Size([])\n",
      "tempModel.cnn.layer3.0.conv3.weight torch.Size([1024, 256, 1, 1])\n",
      "tempModel.cnn.layer3.0.bn3.weight torch.Size([1024])\n",
      "tempModel.cnn.layer3.0.bn3.bias torch.Size([1024])\n",
      "tempModel.cnn.layer3.0.bn3.running_mean torch.Size([1024])\n",
      "tempModel.cnn.layer3.0.bn3.running_var torch.Size([1024])\n",
      "tempModel.cnn.layer3.0.bn3.num_batches_tracked torch.Size([])\n",
      "tempModel.cnn.layer3.0.downsample.0.weight torch.Size([1024, 512, 1, 1])\n",
      "tempModel.cnn.layer3.0.downsample.1.weight torch.Size([1024])\n",
      "tempModel.cnn.layer3.0.downsample.1.bias torch.Size([1024])\n",
      "tempModel.cnn.layer3.0.downsample.1.running_mean torch.Size([1024])\n",
      "tempModel.cnn.layer3.0.downsample.1.running_var torch.Size([1024])\n",
      "tempModel.cnn.layer3.0.downsample.1.num_batches_tracked torch.Size([])\n",
      "tempModel.cnn.layer3.1.conv1.weight torch.Size([256, 1024, 1, 1])\n",
      "tempModel.cnn.layer3.1.bn1.weight torch.Size([256])\n",
      "tempModel.cnn.layer3.1.bn1.bias torch.Size([256])\n",
      "tempModel.cnn.layer3.1.bn1.running_mean torch.Size([256])\n",
      "tempModel.cnn.layer3.1.bn1.running_var torch.Size([256])\n",
      "tempModel.cnn.layer3.1.bn1.num_batches_tracked torch.Size([])\n",
      "tempModel.cnn.layer3.1.conv2.weight torch.Size([256, 256, 3, 3])\n",
      "tempModel.cnn.layer3.1.bn2.weight torch.Size([256])\n",
      "tempModel.cnn.layer3.1.bn2.bias torch.Size([256])\n",
      "tempModel.cnn.layer3.1.bn2.running_mean torch.Size([256])\n",
      "tempModel.cnn.layer3.1.bn2.running_var torch.Size([256])\n",
      "tempModel.cnn.layer3.1.bn2.num_batches_tracked torch.Size([])\n",
      "tempModel.cnn.layer3.1.conv3.weight torch.Size([1024, 256, 1, 1])\n",
      "tempModel.cnn.layer3.1.bn3.weight torch.Size([1024])\n",
      "tempModel.cnn.layer3.1.bn3.bias torch.Size([1024])\n",
      "tempModel.cnn.layer3.1.bn3.running_mean torch.Size([1024])\n",
      "tempModel.cnn.layer3.1.bn3.running_var torch.Size([1024])\n",
      "tempModel.cnn.layer3.1.bn3.num_batches_tracked torch.Size([])\n",
      "tempModel.cnn.layer3.2.conv1.weight torch.Size([256, 1024, 1, 1])\n",
      "tempModel.cnn.layer3.2.bn1.weight torch.Size([256])\n",
      "tempModel.cnn.layer3.2.bn1.bias torch.Size([256])\n",
      "tempModel.cnn.layer3.2.bn1.running_mean torch.Size([256])\n",
      "tempModel.cnn.layer3.2.bn1.running_var torch.Size([256])\n",
      "tempModel.cnn.layer3.2.bn1.num_batches_tracked torch.Size([])\n",
      "tempModel.cnn.layer3.2.conv2.weight torch.Size([256, 256, 3, 3])\n",
      "tempModel.cnn.layer3.2.bn2.weight torch.Size([256])\n",
      "tempModel.cnn.layer3.2.bn2.bias torch.Size([256])\n",
      "tempModel.cnn.layer3.2.bn2.running_mean torch.Size([256])\n",
      "tempModel.cnn.layer3.2.bn2.running_var torch.Size([256])\n",
      "tempModel.cnn.layer3.2.bn2.num_batches_tracked torch.Size([])\n",
      "tempModel.cnn.layer3.2.conv3.weight torch.Size([1024, 256, 1, 1])\n",
      "tempModel.cnn.layer3.2.bn3.weight torch.Size([1024])\n",
      "tempModel.cnn.layer3.2.bn3.bias torch.Size([1024])\n",
      "tempModel.cnn.layer3.2.bn3.running_mean torch.Size([1024])\n",
      "tempModel.cnn.layer3.2.bn3.running_var torch.Size([1024])\n",
      "tempModel.cnn.layer3.2.bn3.num_batches_tracked torch.Size([])\n",
      "tempModel.cnn.layer3.3.conv1.weight torch.Size([256, 1024, 1, 1])\n",
      "tempModel.cnn.layer3.3.bn1.weight torch.Size([256])\n",
      "tempModel.cnn.layer3.3.bn1.bias torch.Size([256])\n",
      "tempModel.cnn.layer3.3.bn1.running_mean torch.Size([256])\n",
      "tempModel.cnn.layer3.3.bn1.running_var torch.Size([256])\n",
      "tempModel.cnn.layer3.3.bn1.num_batches_tracked torch.Size([])\n",
      "tempModel.cnn.layer3.3.conv2.weight torch.Size([256, 256, 3, 3])\n",
      "tempModel.cnn.layer3.3.bn2.weight torch.Size([256])\n",
      "tempModel.cnn.layer3.3.bn2.bias torch.Size([256])\n",
      "tempModel.cnn.layer3.3.bn2.running_mean torch.Size([256])\n",
      "tempModel.cnn.layer3.3.bn2.running_var torch.Size([256])\n",
      "tempModel.cnn.layer3.3.bn2.num_batches_tracked torch.Size([])\n",
      "tempModel.cnn.layer3.3.conv3.weight torch.Size([1024, 256, 1, 1])\n",
      "tempModel.cnn.layer3.3.bn3.weight torch.Size([1024])\n",
      "tempModel.cnn.layer3.3.bn3.bias torch.Size([1024])\n",
      "tempModel.cnn.layer3.3.bn3.running_mean torch.Size([1024])\n",
      "tempModel.cnn.layer3.3.bn3.running_var torch.Size([1024])\n",
      "tempModel.cnn.layer3.3.bn3.num_batches_tracked torch.Size([])\n",
      "tempModel.cnn.layer3.4.conv1.weight torch.Size([256, 1024, 1, 1])\n",
      "tempModel.cnn.layer3.4.bn1.weight torch.Size([256])\n",
      "tempModel.cnn.layer3.4.bn1.bias torch.Size([256])\n",
      "tempModel.cnn.layer3.4.bn1.running_mean torch.Size([256])\n",
      "tempModel.cnn.layer3.4.bn1.running_var torch.Size([256])\n",
      "tempModel.cnn.layer3.4.bn1.num_batches_tracked torch.Size([])\n",
      "tempModel.cnn.layer3.4.conv2.weight torch.Size([256, 256, 3, 3])\n",
      "tempModel.cnn.layer3.4.bn2.weight torch.Size([256])\n",
      "tempModel.cnn.layer3.4.bn2.bias torch.Size([256])\n",
      "tempModel.cnn.layer3.4.bn2.running_mean torch.Size([256])\n",
      "tempModel.cnn.layer3.4.bn2.running_var torch.Size([256])\n",
      "tempModel.cnn.layer3.4.bn2.num_batches_tracked torch.Size([])\n",
      "tempModel.cnn.layer3.4.conv3.weight torch.Size([1024, 256, 1, 1])\n",
      "tempModel.cnn.layer3.4.bn3.weight torch.Size([1024])\n",
      "tempModel.cnn.layer3.4.bn3.bias torch.Size([1024])\n",
      "tempModel.cnn.layer3.4.bn3.running_mean torch.Size([1024])\n",
      "tempModel.cnn.layer3.4.bn3.running_var torch.Size([1024])\n",
      "tempModel.cnn.layer3.4.bn3.num_batches_tracked torch.Size([])\n",
      "tempModel.cnn.layer3.5.conv1.weight torch.Size([256, 1024, 1, 1])\n",
      "tempModel.cnn.layer3.5.bn1.weight torch.Size([256])\n",
      "tempModel.cnn.layer3.5.bn1.bias torch.Size([256])\n",
      "tempModel.cnn.layer3.5.bn1.running_mean torch.Size([256])\n",
      "tempModel.cnn.layer3.5.bn1.running_var torch.Size([256])\n",
      "tempModel.cnn.layer3.5.bn1.num_batches_tracked torch.Size([])\n",
      "tempModel.cnn.layer3.5.conv2.weight torch.Size([256, 256, 3, 3])\n",
      "tempModel.cnn.layer3.5.bn2.weight torch.Size([256])\n",
      "tempModel.cnn.layer3.5.bn2.bias torch.Size([256])\n",
      "tempModel.cnn.layer3.5.bn2.running_mean torch.Size([256])\n",
      "tempModel.cnn.layer3.5.bn2.running_var torch.Size([256])\n",
      "tempModel.cnn.layer3.5.bn2.num_batches_tracked torch.Size([])\n",
      "tempModel.cnn.layer3.5.conv3.weight torch.Size([1024, 256, 1, 1])\n",
      "tempModel.cnn.layer3.5.bn3.weight torch.Size([1024])\n",
      "tempModel.cnn.layer3.5.bn3.bias torch.Size([1024])\n",
      "tempModel.cnn.layer3.5.bn3.running_mean torch.Size([1024])\n",
      "tempModel.cnn.layer3.5.bn3.running_var torch.Size([1024])\n",
      "tempModel.cnn.layer3.5.bn3.num_batches_tracked torch.Size([])\n",
      "tempModel.cnn.layer4.0.conv1.weight torch.Size([512, 1024, 1, 1])\n",
      "tempModel.cnn.layer4.0.bn1.weight torch.Size([512])\n",
      "tempModel.cnn.layer4.0.bn1.bias torch.Size([512])\n",
      "tempModel.cnn.layer4.0.bn1.running_mean torch.Size([512])\n",
      "tempModel.cnn.layer4.0.bn1.running_var torch.Size([512])\n",
      "tempModel.cnn.layer4.0.bn1.num_batches_tracked torch.Size([])\n",
      "tempModel.cnn.layer4.0.conv2.weight torch.Size([512, 512, 3, 3])\n",
      "tempModel.cnn.layer4.0.bn2.weight torch.Size([512])\n",
      "tempModel.cnn.layer4.0.bn2.bias torch.Size([512])\n",
      "tempModel.cnn.layer4.0.bn2.running_mean torch.Size([512])\n",
      "tempModel.cnn.layer4.0.bn2.running_var torch.Size([512])\n",
      "tempModel.cnn.layer4.0.bn2.num_batches_tracked torch.Size([])\n",
      "tempModel.cnn.layer4.0.conv3.weight torch.Size([2048, 512, 1, 1])\n",
      "tempModel.cnn.layer4.0.bn3.weight torch.Size([2048])\n",
      "tempModel.cnn.layer4.0.bn3.bias torch.Size([2048])\n",
      "tempModel.cnn.layer4.0.bn3.running_mean torch.Size([2048])\n",
      "tempModel.cnn.layer4.0.bn3.running_var torch.Size([2048])\n",
      "tempModel.cnn.layer4.0.bn3.num_batches_tracked torch.Size([])\n",
      "tempModel.cnn.layer4.0.downsample.0.weight torch.Size([2048, 1024, 1, 1])\n",
      "tempModel.cnn.layer4.0.downsample.1.weight torch.Size([2048])\n",
      "tempModel.cnn.layer4.0.downsample.1.bias torch.Size([2048])\n",
      "tempModel.cnn.layer4.0.downsample.1.running_mean torch.Size([2048])\n",
      "tempModel.cnn.layer4.0.downsample.1.running_var torch.Size([2048])\n",
      "tempModel.cnn.layer4.0.downsample.1.num_batches_tracked torch.Size([])\n",
      "tempModel.cnn.layer4.1.conv1.weight torch.Size([512, 2048, 1, 1])\n",
      "tempModel.cnn.layer4.1.bn1.weight torch.Size([512])\n",
      "tempModel.cnn.layer4.1.bn1.bias torch.Size([512])\n",
      "tempModel.cnn.layer4.1.bn1.running_mean torch.Size([512])\n",
      "tempModel.cnn.layer4.1.bn1.running_var torch.Size([512])\n",
      "tempModel.cnn.layer4.1.bn1.num_batches_tracked torch.Size([])\n",
      "tempModel.cnn.layer4.1.conv2.weight torch.Size([512, 512, 3, 3])\n",
      "tempModel.cnn.layer4.1.bn2.weight torch.Size([512])\n",
      "tempModel.cnn.layer4.1.bn2.bias torch.Size([512])\n",
      "tempModel.cnn.layer4.1.bn2.running_mean torch.Size([512])\n",
      "tempModel.cnn.layer4.1.bn2.running_var torch.Size([512])\n",
      "tempModel.cnn.layer4.1.bn2.num_batches_tracked torch.Size([])\n",
      "tempModel.cnn.layer4.1.conv3.weight torch.Size([2048, 512, 1, 1])\n",
      "tempModel.cnn.layer4.1.bn3.weight torch.Size([2048])\n",
      "tempModel.cnn.layer4.1.bn3.bias torch.Size([2048])\n",
      "tempModel.cnn.layer4.1.bn3.running_mean torch.Size([2048])\n",
      "tempModel.cnn.layer4.1.bn3.running_var torch.Size([2048])\n",
      "tempModel.cnn.layer4.1.bn3.num_batches_tracked torch.Size([])\n",
      "tempModel.cnn.layer4.2.conv1.weight torch.Size([512, 2048, 1, 1])\n",
      "tempModel.cnn.layer4.2.bn1.weight torch.Size([512])\n",
      "tempModel.cnn.layer4.2.bn1.bias torch.Size([512])\n",
      "tempModel.cnn.layer4.2.bn1.running_mean torch.Size([512])\n",
      "tempModel.cnn.layer4.2.bn1.running_var torch.Size([512])\n",
      "tempModel.cnn.layer4.2.bn1.num_batches_tracked torch.Size([])\n",
      "tempModel.cnn.layer4.2.conv2.weight torch.Size([512, 512, 3, 3])\n",
      "tempModel.cnn.layer4.2.bn2.weight torch.Size([512])\n",
      "tempModel.cnn.layer4.2.bn2.bias torch.Size([512])\n",
      "tempModel.cnn.layer4.2.bn2.running_mean torch.Size([512])\n",
      "tempModel.cnn.layer4.2.bn2.running_var torch.Size([512])\n",
      "tempModel.cnn.layer4.2.bn2.num_batches_tracked torch.Size([])\n",
      "tempModel.cnn.layer4.2.conv3.weight torch.Size([2048, 512, 1, 1])\n",
      "tempModel.cnn.layer4.2.bn3.weight torch.Size([2048])\n",
      "tempModel.cnn.layer4.2.bn3.bias torch.Size([2048])\n",
      "tempModel.cnn.layer4.2.bn3.running_mean torch.Size([2048])\n",
      "tempModel.cnn.layer4.2.bn3.running_var torch.Size([2048])\n",
      "tempModel.cnn.layer4.2.bn3.num_batches_tracked torch.Size([])\n",
      "tempModel.cnn.fc.weight torch.Size([1000, 2048])\n",
      "tempModel.cnn.fc.bias torch.Size([1000])\n",
      "tempModel.scoreConv.conv1.weight torch.Size([8, 1, 7])\n",
      "tempModel.scoreConv.conv1.bias torch.Size([8])\n",
      "tempModel.scoreConv.conv2.weight torch.Size([1, 8, 1])\n",
      "tempModel.scoreConv.conv2.bias torch.Size([1])\n",
      "tempModel.scoreConv.layers.0.weight torch.Size([8, 1, 7])\n",
      "tempModel.scoreConv.layers.0.bias torch.Size([8])\n",
      "tempModel.scoreConv.layers.2.weight torch.Size([1, 8, 1])\n",
      "tempModel.scoreConv.layers.2.bias torch.Size([1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "if not os.path.exists(\"./modelresnet50_biconvScoInitAtt_epoch462\"):\n",
    "    subprocess.call(\"gdown https://drive.google.com/uc?id=1WaAGxH4YzvNwvVP2Fjm4xtdgHIMRks1S\",shell=True)\n",
    "    \n",
    "model = modelBuilder.SceneDet(temp_model=\"resnet50\",featModelName=\"resnet50\",cuda=torch.cuda.is_available(),\\\n",
    "                            scoreConvWindSize=7,scoreConvBiLay=True,scoreConvChan=8,scoreConvAtt=True)\n",
    "\n",
    "\n",
    "params = torch.load(\"./modelresnet50_biconvScoInitAtt_epoch462\",map_location=torch.device('cpu'))\n",
    "\n",
    "state_dict = {k.replace(\"module.cnn.\",\"cnn.module.\")\\\n",
    "               .replace(\"scoreConv.weight\",\"scoreConv.layers.weight\")\\\n",
    "               .replace(\"scoreConv.bias\",\"scoreConv.layers.bias\")\\\n",
    "               .replace(\".module\",\"\") : v for k,v in params.items()}\n",
    "\n",
    "paramToRemove = []\n",
    "for param in state_dict.keys():\n",
    "    if param.find(\"frameAtt\") != -1:\n",
    "        paramToRemove.append(param)\n",
    "for param in paramToRemove:\n",
    "    state_dict.pop(param)\n",
    "\n",
    "for paramKey in state_dict.keys():\n",
    "    print(paramKey,state_dict[paramKey].size())\n",
    "    \n",
    "model.load_state_dict(state_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Computing the scene change scores\n",
    "\n",
    "This computes the scene change score (or probability) given by the model to each shot of the movie. It plots the score for each shot of the video, along with the ground-truth scene change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([288])\n",
      "(288, 2)\n"
     ]
    }
   ],
   "source": [
    "video = pims.Video(videoPath)\n",
    "\n",
    "if not os.path.exists(\"res50_res50_biconvScoInitAtt_youtLarg_7_epoch462_{}.csv\".format(videoName)):\n",
    "\n",
    "    N = 10 #The model reads the first N shots at the same time, then the next N shots, etc.\n",
    "    imgSize = 299\n",
    "\n",
    "\n",
    "    #This separates the frame indexes into several batches which size is close to N\n",
    "    split_keyFrameInds = np.array_split(keyFrameInds,len(keyFrameInds)//N+(len(keyFrameInds)%N != 0))\n",
    "\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    preproc = transforms.Compose([transforms.ToPILImage(),transforms.CenterCrop(imgSize),transforms.ToTensor(),normalize])\n",
    "\n",
    "    allScores = []\n",
    "\n",
    "    for frameInds in split_keyFrameInds:\n",
    "\n",
    "        frameSeq = torch.cat(list(map(lambda x:preproc(video[x]).unsqueeze(0),np.array(frameInds))),dim=0).unsqueeze(0)\n",
    "        scores = model(frameSeq,None).data\n",
    "        allScores.append(scores.squeeze(0))\n",
    "\n",
    "    scores = torch.cat(allScores,dim=0)\n",
    "    print(scores.shape)\n",
    "    scores = np.concatenate((keyFrameInds[:,np.newaxis],scores[:,np.newaxis]),axis=1)\n",
    "    print(scores.shape)\n",
    "\n",
    "    np.savetxt(\"res50_res50_biconvScoInitAtt_youtLarg_7_epoch462_{}.csv\".format(videoName),scores)\n",
    "else:\n",
    "    scores = np.genfromtxt(\"res50_res50_biconvScoInitAtt_youtLarg_7_epoch462_{}.csv\".format(videoName))\n",
    "    \n",
    "plt.figure(figsize=(20,5))\n",
    "\n",
    "#Decision threshold\n",
    "plt.hlines([0.5],0,len(scores),linewidths=3,color='black')\n",
    "\n",
    "#Plot the scores\n",
    "plt.plot(np.arange(len(scores)),scores[:,1],color=\"blue\",label=\"Scene change score\")\n",
    "\n",
    "#Plot the GT\n",
    "if not os.path.exists(\"Sintel_targ.csv\"):\n",
    "    subprocess.call(\"gdown https://drive.google.com/uc?id=129dQBUUzYjdlJYoy2nOX8yLSccS7Izam\",shell=True)\n",
    "gt = np.genfromtxt(\"Sintel_targ.csv\")\n",
    "plt.vlines(gt.nonzero(),0,1,linewidths=10,color='orange')\n",
    "\n",
    "plt.xlabel(\"Time (shot index)\")\n",
    "plt.ylabel(\"Probability of scene change\")\n",
    "plt.savefig(\"scores.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The black line is the decision threshold : above it the score is high enough and the shot is predicted as a scene change.\n",
    "The orange line shows the ground truth scene changes\n",
    "The blue line shows the score produced by the model.\n",
    "\n",
    "We can see that there are almost all scene change are correctly captured and there only a few false positive.\n",
    "Note that around shots 90-100, the model predicted two scene changes very confidently although there is no real scene change. By visualising the shots just below, we might suspect that this comes from the strong visual difference between shots 90-93 and shots 94-96"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Showing the scene changes\n",
    "\n",
    "This cell shows a few scene change predicted by the model. It shows a sequence of key-frames centered around a predicted scene change. The red line indicates between which shots the scene change is predicted to occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/E144069X/.local/lib/python3.6/site-packages/ipykernel_launcher.py:42: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n"
     ]
    }
   ],
   "source": [
    "decisionThres = 0.5 # If a shot score is above this threshold, it will be considered as a scene change\n",
    "\n",
    "#The index of the shots in which a scene change occurs\n",
    "sceneChange_shotIndex = (scores[:,1]>decisionThres).nonzero()[0]\n",
    "#The index of the frames in which a scene change occurs\n",
    "sceneChange_frameIndex = shotBounds[sceneChange_shotIndex][:,0]\n",
    "\n",
    "imgSize = 20\n",
    "contextShots = 3\n",
    "columns = 2*contextShots\n",
    "rows = 8\n",
    "\n",
    "frame = video[0]\n",
    "ratio = frame.shape[1]/frame.shape[0] - 2\n",
    "\n",
    "plt.figure(figsize=(imgSize*ratio*contextShots, imgSize))\n",
    "\n",
    "gs1 = gridspec.GridSpec(rows, columns)\n",
    "gs1.update(wspace=0.0, hspace=0.0)\n",
    "\n",
    "#Each line shows one scene change (according to the model)\n",
    "for i in range(rows):\n",
    "\n",
    "    #Plot some key-frames before the scene change\n",
    "    for j in range(contextShots):\n",
    "        precKeyFrameInd = shotBounds[sceneChange_shotIndex[i]+j-contextShots].mean().astype(int)\n",
    "        plt.subplot(gs1[i*columns+j])\n",
    "        plt.title(\"Shot \"+str(sceneChange_shotIndex[i]+j-contextShots))\n",
    "        plt.axis('off')\n",
    "        plt.imshow(video[precKeyFrameInd])\n",
    "       \n",
    "    plt.vlines([video[precKeyFrameInd].shape[1]],0,video[precKeyFrameInd].shape[0],color=\"orange\",linewidths=20)\n",
    "    \n",
    "    #Plot some key-frames after the scene change\n",
    "    for j in range(contextShots):\n",
    "        keyFrameInd = shotBounds[sceneChange_shotIndex[i]+j].mean().astype(int)\n",
    "        plt.subplot(gs1[i*columns+j+contextShots])\n",
    "        plt.title(\"Shot \"+str(sceneChange_shotIndex[i]+j))\n",
    "        plt.axis('off')\n",
    "        plt.imshow(video[keyFrameInd])\n",
    "\n",
    "plt.show()\n",
    "plt.savefig(\"{}_sceneChangeExamples.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each line shows some key frames around the scene change predicted by the model.\n",
    "Only the first 8 predicted scene change are shown.\n",
    "\n",
    "The third and fourth scene change detected are false positives. We can suspect the model has predicted a change because of the strong visual difference between the shots 90-93 and the shots 94-96."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
